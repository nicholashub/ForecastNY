{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Smooth Optimization Using pyOpt and AlgoPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses package [`pyOpt`](http://www.pyopt.org/index.html) for optimization, and package [`algopy`](https://pythonhosted.org/algopy/) to evaluate gradients (subgradients) for non-smooth functions. Specifically, we use [`SOLVOPT`](http://www.pyopt.org/reference/optimizers.solvopt.html) minimizer in `pyOpt` to implement a modified version of Shor’s r–algorithm on non-smooth functions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyOpt` has the following dependencies: Python 2.4+, Numpy 1.0+, Swig 1.3+, c/FORTRAN compiler (compatible with f2py). Here Python 2.7 and numpy 1.10.1 are used (newest version of `numpy` may not work on `pyOpt` for unsolved compatibility issues). See `pyOpt` [documentation](http://www.pyopt.org/install.html#installation) for details on installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.13 |Continuum Analytics, Inc.| (default, Dec 20 2016, 23:05:08) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]\n",
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "import sys, numpy\n",
    "\n",
    "print sys.version\n",
    "print numpy.version.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import algopy\n",
    "import numpy as np\n",
    "import pyOpt\n",
    "from pyOpt import Optimization\n",
    "from pyOpt import SOLVOPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Automatic Differentiation Using AlgoPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AD():\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self.f = f     \n",
    "\n",
    "    # Forward Mode Derivative Evaluations\n",
    "    def grad_forward(self, x):\n",
    "        x = algopy.UTPM.init_jacobian(x)\n",
    "        return algopy.UTPM.extract_jacobian(self.f(x))\n",
    "    \n",
    "    def hess_forward(self, x):\n",
    "        x = algopy.UTPM.init_hessian(x)\n",
    "        return algopy.UTPM.extract_hessian(len(x), self.f(x))\n",
    "\n",
    "    def hess_vector_forward(self, x, v):\n",
    "        x = algopy.UTPM.init_hess_vec(x, v)\n",
    "        return algopy.UTPM.extract_hess_vec(len(x), self.f(x))\n",
    "    \n",
    "    # Reverse Mode Derivative Evaluations\n",
    "    def trace_eval_f(self, x):\n",
    "        cg = algopy.CGraph()\n",
    "        x = algopy.Function(x)\n",
    "        y = self.f(x)\n",
    "        cg.trace_off()\n",
    "        cg.independentFunctionList = [x]\n",
    "        cg.dependentFunctionList = [y]\n",
    "        self.cg = cg\n",
    "\n",
    "    def grad_reverse(self, x):\n",
    "        self.trace_eval_f(x)\n",
    "        return self.cg.gradient(x)\n",
    "\n",
    "    def hess_reverse(self, x):\n",
    "        self.trace_eval_f(x)\n",
    "        return self.cg.hessian(x)\n",
    "    \n",
    "    def hess_vector_reverse(self, x, v):\n",
    "        self.trace_eval_f(x)\n",
    "        return self.cg.hess_vec(x,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Smooth Test Function: Absolute Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to optimize function $f(\\mathbf{x})=\\sum_{i=1}^{10}|{x_i}|$ where its minimum locates at $\\mathbf{x}=\\mathbf{0}$. Subgradients are computed through `AlgoPy`, and they are provided as user-defined gradient function to the minimizer. Alternatively, we can also compute subgradients using numerical differentiation by setting `sens_type='FD'` in function `solveopt`, though finite differences may not provide machine level precision as does automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization Problem -- Sum of N Absolute Value Functions\n",
      "================================================================================\n",
      "\n",
      "        Objective Function: objfunc\n",
      "\n",
      "    Objectives:\n",
      "        Name        Value        Optimum\n",
      "\t     f               0             0\n",
      "\n",
      "\tVariables (c - continuous, i - integer, d - discrete):\n",
      "        Name    Type       Value       Lower Bound  Upper Bound\n",
      "\t    x_0       c\t      0.000000           -inf          inf \n",
      "\t    x_1       c\t      1.000000           -inf          inf \n",
      "\t    x_2       c\t      2.000000           -inf          inf \n",
      "\t    x_3       c\t      3.000000           -inf          inf \n",
      "\t    x_4       c\t      4.000000           -inf          inf \n",
      "\t    x_5       c\t      5.000000           -inf          inf \n",
      "\t    x_6       c\t      6.000000           -inf          inf \n",
      "\t    x_7       c\t      7.000000           -inf          inf \n",
      "\t    x_8       c\t      8.000000           -inf          inf \n",
      "\t    x_9       c\t      9.000000           -inf          inf \n",
      "\n",
      "\n",
      "SOLVOPT Solution to Sum of N Absolute Value Functions\n",
      "================================================================================\n",
      "\n",
      "        Objective Function: objfunc\n",
      "\n",
      "    Solution: \n",
      "--------------------------------------------------------------------------------\n",
      "    Total Time:                    0.5313\n",
      "    Total Function Evaluations:       391\n",
      "    Sensitivities: <function gradfunc at 0x113527c80>\n",
      "\n",
      "    Objectives:\n",
      "        Name        Value        Optimum\n",
      "\t     f     4.14288e-08             0\n",
      "\n",
      "\tVariables (c - continuous, i - integer, d - discrete):\n",
      "        Name    Type       Value       Lower Bound  Upper Bound\n",
      "\t    x_0       c\t      0.000000           -inf          inf \n",
      "\t    x_1       c\t     -0.000000           -inf          inf \n",
      "\t    x_2       c\t     -0.000000           -inf          inf \n",
      "\t    x_3       c\t      0.000000           -inf          inf \n",
      "\t    x_4       c\t     -0.000000           -inf          inf \n",
      "\t    x_5       c\t      0.000000           -inf          inf \n",
      "\t    x_6       c\t      0.000000           -inf          inf \n",
      "\t    x_7       c\t      0.000000           -inf          inf \n",
      "\t    x_8       c\t     -0.000000           -inf          inf \n",
      "\t    x_9       c\t      0.000000           -inf          inf \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sum_abs(x):\n",
    "    return algopy.sum(algopy.absolute(x))\n",
    "\n",
    "def objfunc(x):\n",
    "    f = sum_abs(x)\n",
    "    g = []\n",
    "    fail = 0\n",
    "    return f, g, fail\n",
    "\n",
    "ad = AD(sum_abs)\n",
    "\n",
    "def gradfunc(x, f, g):\n",
    "    g_obj = ad.grad_reverse(x) # Use reverse mode to evaluate subgradient (we can also use forward mode)\n",
    "    g_obj = list(g_obj)\n",
    "    \n",
    "    g_con = []\n",
    "    fail = 0\n",
    "    return g_obj, g_con, fail\n",
    "\n",
    "# Optimization setup\n",
    "opt_prob = Optimization('Sum of N Absolute Value Functions', objfunc)\n",
    "opt_prob.addVarGroup('x', 10, type='c', value=np.arange(10, dtype=np.float), lower=-np.inf, upper=np.inf)\n",
    "opt_prob.addObj('f')\n",
    "print opt_prob\n",
    "\n",
    "# Call SOLVOPT minimizer to implement Shor's r-algorithm\n",
    "solvopt = SOLVOPT()\n",
    "solvopt.setOption('iprint', -1)\n",
    "solvopt(opt_prob, sens_type=gradfunc)\n",
    "print opt_prob.solution(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Smooth Test Function: Shor's Piece-Wise Quadratic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's minimize a more complicated function: Shor's piece-wise quadratic function. The unconstrained optimization problem is defined as,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\min_{\\mathbf{x}}\\big\\{f(\\mathbf{x}):x \\in\\ {\\rm I\\!R}^n \\big\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x})=\\max{\\big\\{ \\phi_i(\\mathbf{x}): i=1,2,\\dots,m \\big\\}}, \\quad \\phi_i(\\mathbf{x})=b_i\\sum_{j=1}^{n}(x_j-a_{ij})^2, \\mathbf{x}=(x_1, \\dots, x_n)\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\mathbf{b}=(b_i) \\text{ is a given m vector, } \\mathbf{A}=(a_{ij}), i=1,\\dots, m, \\text{ } j=1,\\dots, n, \\text{ is a given } m \\times n \\text{ matrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization Problem -- Shor's Piece-Wise Quadratic Function\n",
      "================================================================================\n",
      "\n",
      "        Objective Function: objfunc\n",
      "\n",
      "    Objectives:\n",
      "        Name        Value        Optimum\n",
      "\t     f               0             0\n",
      "\n",
      "\tVariables (c - continuous, i - integer, d - discrete):\n",
      "        Name    Type       Value       Lower Bound  Upper Bound\n",
      "\t    x_0       c\t     -1.000000           -inf          inf \n",
      "\t    x_1       c\t      1.000000           -inf          inf \n",
      "\t    x_2       c\t     -1.000000           -inf          inf \n",
      "\t    x_3       c\t      1.000000           -inf          inf \n",
      "\t    x_4       c\t     -1.000000           -inf          inf \n",
      "\n",
      "\n",
      "SOLVOPT Solution to Shor's Piece-Wise Quadratic Function\n",
      "================================================================================\n",
      "\n",
      "        Objective Function: objfunc\n",
      "\n",
      "    Solution: \n",
      "--------------------------------------------------------------------------------\n",
      "    Total Time:                    2.9959\n",
      "    Total Function Evaluations:       212\n",
      "    Sensitivities: <function gradfunc at 0x113660050>\n",
      "\n",
      "    Objectives:\n",
      "        Name        Value        Optimum\n",
      "\t     f         22.6002             0\n",
      "\n",
      "\tVariables (c - continuous, i - integer, d - discrete):\n",
      "        Name    Type       Value       Lower Bound  Upper Bound\n",
      "\t    x_0       c\t      1.124364           -inf          inf \n",
      "\t    x_1       c\t      0.979458           -inf          inf \n",
      "\t    x_2       c\t      1.477714           -inf          inf \n",
      "\t    x_3       c\t      0.920252           -inf          inf \n",
      "\t    x_4       c\t      1.124286           -inf          inf \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def shor(x):\n",
    "    A = np.array([[0, 0, 0, 0, 0],\n",
    "                  [2, 1, 1, 1, 3],\n",
    "                  [1, 2, 1, 1, 2],\n",
    "                  [1, 4, 1, 2, 2],\n",
    "                  [3, 2, 1, 0, 1],\n",
    "                  [0, 2, 1, 0, 1],\n",
    "                  [1, 1, 1, 1, 1],\n",
    "                  [1, 0, 1, 2, 1],\n",
    "                  [0, 0, 2, 1, 0],\n",
    "                  [1, 1, 2, 0, 0]])\n",
    "    b = np.array([1, 5, 10, 2, 4, 3, 1.7, 2.5, 6, 4.5])\n",
    "    res = 0\n",
    "    for k in range(b.size):\n",
    "        temp = b[k] * algopy.sum((x - A[k, :])**2)\n",
    "        res = 0.5 * (temp + res + algopy.absolute(temp - res)) # max(x, y) = (x+y+abs(x-y)) / 2\n",
    "    return res\n",
    "\n",
    "def objfunc(x):\n",
    "    f = shor(x)\n",
    "    g = []\n",
    "    fail = 0\n",
    "    return f, g, fail\n",
    "\n",
    "ad = AD(shor)\n",
    "\n",
    "def gradfunc(x, f, g):\n",
    "    g_obj = ad.grad_reverse(x) # Use reverse mode to evaluate subgradient (we can also use forward mode)\n",
    "    g_obj = list(g_obj)\n",
    "    g_con = []\n",
    "    fail = 0\n",
    "    return g_obj, g_con, fail\n",
    "\n",
    "# Optimization setup\n",
    "opt_prob = Optimization(\"Shor's Piece-Wise Quadratic Function\", objfunc)\n",
    "opt_prob.addVarGroup('x', 5, type='c', value=np.array([-1, 1, -1, 1, -1]), lower=-np.inf, upper=np.inf)\n",
    "opt_prob.addObj('f')\n",
    "print opt_prob\n",
    "\n",
    "# Call SOLVOPT minimizer to implement Shor's r-algorithm\n",
    "solvopt = SOLVOPT()\n",
    "solvopt.setOption('iprint', -1)\n",
    "solvopt(opt_prob, sens_type=gradfunc)\n",
    "print opt_prob.solution(0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
